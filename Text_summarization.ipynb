{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "head.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "dl6dw7KYM3-X",
        "colab_type": "code",
        "outputId": "cdfc4506-1ebd-45db-a8f8-5da725b03692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "import numpy as np\n",
        "import io\n",
        "import nltk\n",
        "import pickle\n",
        "from pickle import dump, load\n",
        "from google.colab import files\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import tensor_array_ops\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 21.0MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 3.3MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 4.8MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 3.0MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 3.7MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 4.4MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 5.1MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 5.7MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 6.3MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 4.0MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 4.0MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 4.8MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 4.4MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 6.2MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 6.1MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 5.9MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 5.9MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 5.2MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 5.2MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 6.9MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 6.7MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 7.4MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 7.4MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 5.6MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 5.7MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 5.1MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 5.1MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 5.0MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 4.4MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 5.9MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 5.8MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 5.7MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 6.5MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 8.4MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 8.4MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 10.8MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 9.2MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 12.3MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 17.6MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 15.7MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 18.1MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 15.9MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 15.7MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 17.3MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 17.2MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 17.8MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 22.8MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 19.7MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 21.1MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 24.3MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 24.3MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 21.0MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 18.8MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 20.0MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 16.5MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 15.6MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 16.7MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 18.8MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 18.9MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 17.7MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 15.8MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 21.1MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 24.2MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 19.7MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 21.9MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 23.0MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 23.1MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 23.2MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 21.6MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 24.2MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 21.5MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 20.0MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 20.1MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 24.4MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 28.7MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 26.9MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 22.5MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 22.5MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 19.2MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 19.1MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 25.3MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 22.1MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 22.2MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 21.0MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 16.5MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 17.6MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 20.3MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 18.7MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 23.3MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 23.4MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 23.4MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 28.6MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 28.6MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 31.7MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 27.5MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 27.4MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 14.5MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hTensorFlow Version: 1.13.1\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "IwNHr4rzNYIy",
        "colab_type": "code",
        "outputId": "dc386bd8-a2fa-4225-dd18-0e1d84c90bf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "# load weights\n",
        "link='https://drive.google.com/open?id=1DJFvHYEQY_cAKh37yTY80iOzviRTsimw'\n",
        "fluff, id = link.split('=')\n",
        "print (id)\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('best_model.h5.data-00000-of-00001')\n",
        "\n",
        "link='https://drive.google.com/open?id=1kV4hl4ujLDysxchJJY94QsZPOFWXBEsM'\n",
        "fluff, id = link.split('=')\n",
        "print (id)\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('best_model.h5.meta')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1DJFvHYEQY_cAKh37yTY80iOzviRTsimw\n",
            "1kV4hl4ujLDysxchJJY94QsZPOFWXBEsM\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "qL1yE9HoeaKH",
        "colab_type": "code",
        "outputId": "0a307879-e7b5-49b6-d1e2-15f6ab6a7e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "link='https://drive.google.com/open?id=14S3NOL1Cc6NT214Fl0GIbjsu9NjXEFm1'\n",
        "fluff, id = link.split('=')\n",
        "print (id)\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('vocab_to_int.pkl')\n",
        "vocab_to_int = load(open('vocab_to_int.pkl', 'rb'))\n",
        "print(type(vocab_to_int))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14S3NOL1Cc6NT214Fl0GIbjsu9NjXEFm1\n",
            "<class 'dict'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4mALMxoCicqB",
        "colab_type": "code",
        "outputId": "d8353e61-a90b-401e-d929-c98cf9fe4e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "link='https://drive.google.com/open?id=19UT1XqkcR7kV6WIO3u9FYmFOqFy-FdyI'\n",
        "fluff, id = link.split('=')\n",
        "print (id)\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('int_to_vocab.pkl')\n",
        "int_to_vocab = load(open('int_to_vocab.pkl', 'rb'))\n",
        "print(type(int_to_vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19UT1XqkcR7kV6WIO3u9FYmFOqFy-FdyI\n",
            "<class 'dict'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p93xrVXdYVzp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "contractions = {\n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"\n",
        "}\n",
        "def clean_text(text, remove_stopwords=True):\n",
        "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
        "\n",
        "    # Convert words to lower case\n",
        "    text = text.lower()\n",
        "\n",
        "    # Replace contractions with their longer forms\n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "\n",
        "    # Format words and remove unwanted characters\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text)\n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'>', ' ', text)\n",
        "    text = re.sub(r'<br', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "\n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        text = text.split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "        text = \" \".join(text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MuUu_a-PQ5MX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 64\n",
        "\n",
        "def text_to_seq(text):\n",
        "    '''Prepare the text for the model'''\n",
        "    \n",
        "    text = clean_text(text)\n",
        "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZhoDMzDXNfLa",
        "colab_type": "code",
        "outputId": "1c2ee6af-541a-45dd-da1d-d86d02bbbe42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        }
      },
      "cell_type": "code",
      "source": [
        "input_sentences=[\"wife use everyday dinners crystal clean white finishing salt 33 lower sodium table salt 2 magnesium calcium super healthy used water workouts little pinch good electrolyte boost big fan product honestly cannot recommend enough reminds higher quality fleur de sel\",\n",
        "              \"worst poduct, dont buy,bad food product i did not like it \",\n",
        "                  \"bad food product\",\n",
        "                 \"Great taffy at a great price.There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\",\n",
        "                 \"Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product it as Jumbo bad worst.\",\n",
        "                 \" bought local farmer specialty market thoroughly impressed affinity cheeses especially variety made easier pick quality product rich flavorful\",\n",
        "                 \"excellent product long lasting hard candy sugar free great taste without funny taste sugar free candies\"\n",
        "                 ,\"far delicious garlic ever tasted kids love go like candy happy good us\"\n",
        "                 \n",
        "]\n",
        " \n",
        "generagte_summary_length=[]\n",
        "for i in range(len(input_sentences)):\n",
        "        generagte_summary_length.append(10)\n",
        "\n",
        "texts = [text_to_seq(input_sentence) for input_sentence in input_sentences]\n",
        "checkpoint = \"./best_model.h5\"\n",
        "if type(generagte_summary_length) is list:\n",
        "    if len(input_sentences)!=len(generagte_summary_length):\n",
        "        raise Exception(\"[Error] makeSummaries parameter generagte_summary_length must be same length as input_sentences or an integer\")\n",
        "    generagte_summary_length_list = generagte_summary_length\n",
        "else:\n",
        "    generagte_summary_length_list = [generagte_summary_length] * len(texts)\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    for i, text in enumerate(texts):\n",
        "        generagte_summary_length = generagte_summary_length_list[i]\n",
        "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                          summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)], \n",
        "                                          text_length: [len(text)]*batch_size,\n",
        "                                          keep_prob: 1.0})[0] \n",
        "        # Remove the padding from the summaries\n",
        "        pad = vocab_to_int[\"<PAD>\"] \n",
        "        print('- Review:\\n\\r {}'.format(input_sentences[i]))\n",
        "        print('- Summary:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./best_model.h5\n",
            "- Review:\n",
            " wife use everyday dinners crystal clean white finishing salt 33 lower sodium table salt 2 magnesium calcium super healthy used water workouts little pinch good electrolyte boost big fan product honestly cannot recommend enough reminds higher quality fleur de sel\n",
            "- Summary:\n",
            " great crystal\n",
            "\n",
            "\n",
            "- Review:\n",
            " worst poduct, dont buy,bad food product i did not like it \n",
            "- Summary:\n",
            " do not buy these bags from amazon com\n",
            "\n",
            "\n",
            "- Review:\n",
            " bad food product\n",
            "- Summary:\n",
            " not happy\n",
            "\n",
            "\n",
            "- Review:\n",
            " Great taffy at a great price.There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
            "- Summary:\n",
            " great product\n",
            "\n",
            "\n",
            "- Review:\n",
            " Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product it as Jumbo bad worst.\n",
            "- Summary:\n",
            " what is wrong with the product not size\n",
            "\n",
            "\n",
            "- Review:\n",
            "  bought local farmer specialty market thoroughly impressed affinity cheeses especially variety made easier pick quality product rich flavorful\n",
            "- Summary:\n",
            " tasty but very addictive\n",
            "\n",
            "\n",
            "- Review:\n",
            " excellent product long lasting hard candy sugar free great taste without funny taste sugar free candies\n",
            "- Summary:\n",
            " tasty and light\n",
            "\n",
            "\n",
            "- Review:\n",
            " far delicious garlic ever tasted kids love go like candy happy good us\n",
            "- Summary:\n",
            " yummy and not healthy\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}